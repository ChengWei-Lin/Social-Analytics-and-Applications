{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: [Topic Modelling](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>Categories</th>\n",
       "      <th>google_score</th>\n",
       "      <th>textblob_score</th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nice Designs</td>\n",
       "      <td>Praise on product</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Looks good! Price?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lovely shoes. It looks like a hole in the welt...</td>\n",
       "      <td>Praise on product</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Beautiful</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How much for this piece?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kindly DM what size available</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beautiful colour</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  \\\n",
       "0                                             thanks   \n",
       "1                                       Nice Designs   \n",
       "2                                          How much?   \n",
       "3                                 Looks good! Price?   \n",
       "4                                          Fantastic   \n",
       "5  Lovely shoes. It looks like a hole in the welt...   \n",
       "6                                          Beautiful   \n",
       "7                           How much for this piece?   \n",
       "8                      kindly DM what size available   \n",
       "9                                   Beautiful colour   \n",
       "\n",
       "                             Categories  google_score  textblob_score  \\\n",
       "0                          Nice content           0.3           0.200   \n",
       "1                     Praise on product           0.8           0.600   \n",
       "2  Query about price or oder or product           0.0           0.200   \n",
       "3  Query about price or oder or product           0.3           0.875   \n",
       "4                          Nice content           0.9           0.400   \n",
       "5                     Praise on product           0.1           0.250   \n",
       "6                          Nice content           0.8           0.850   \n",
       "7  Query about price or oder or product          -0.1           0.200   \n",
       "8  Query about price or oder or product           0.0           0.500   \n",
       "9                          Nice content           0.8           0.850   \n",
       "\n",
       "   avg_score  \n",
       "0     0.2500  \n",
       "1     0.7000  \n",
       "2     0.1000  \n",
       "3     0.5875  \n",
       "4     0.6500  \n",
       "5     0.1750  \n",
       "6     0.8250  \n",
       "7     0.0500  \n",
       "8     0.2500  \n",
       "9     0.8250  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('insta data/comment_noemo_score.xlsx', encoding='latin-1')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['comments'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "#Text Lemmatization\n",
    "#As we are using wordnet Lemmatizer and the the standard NLTK pos tags are treebank tags, we need to convert the treebank tag\n",
    "#to wordnet tags. \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    lemmatized_wordlist=[]\n",
    "    for w in tagged_words:\n",
    "        wordnettag=get_wordnet_pos(w[1])\n",
    "        if wordnettag == '':\n",
    "            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower())\n",
    "        else:\n",
    "            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower(),pos=wordnettag)\n",
    "        if w[0].istitle():\n",
    "            lemmatizedword = lemmatizedword.capitalize()\n",
    "        elif w[0].upper()==w[0]:\n",
    "            lemmatizedword = lemmatizedword.upper()\n",
    "        else:\n",
    "            lemmatizedword = lemmatizedword\n",
    "        lemmatized_wordlist.append(lemmatizedword)\n",
    "            \n",
    "    return lemmatized_wordlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = ['hi', 'seamlessbespoke', \"'ll\", \"'re\",  \"'d\", 'congratulations', \n",
    "               'congrats', 'allureisourduty', 'pls', 'hey', 'make', 'look', 'post', 'pic', 'really', 'thanks', 'ri', 'veri',\n",
    "              'much', 'one', 'please', 'would', 'keep', 'wait']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    #lower case\n",
    "    x = x.lower()\n",
    "    \n",
    "    #remove useless noise\n",
    "#     for lol in remove_list:\n",
    "#         x = x.replace(lol, '')\n",
    "    words = nltk.word_tokenize(x)\n",
    "#     print(words)\n",
    "    words = [w for w in words if w not in remove_list]\n",
    "#     print(words)\n",
    "#     #remove punctuation\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(x)\n",
    "#     tokens = words[3:]\n",
    "    \n",
    "    #remove stopwords\n",
    "    filtered_words = [w for w in words if  w not in stop]\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "#     #autocorrect spelling error\n",
    "#     spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "#     text = \" \".join(spells)\n",
    "    \n",
    "    #lemmatization\n",
    "    text = lemmatization(text)\n",
    "    text = [w for w in text if w not in remove_list]\n",
    "#     print(text)\n",
    "    text = \" \".join(text)\n",
    "    output = text\n",
    "    \n",
    "    #remove digits\n",
    "#     output = ''.join(c for c in text if not c.isdigit())\n",
    "    \n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nice gent'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df['comments'][134])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['comments'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>Categories</th>\n",
       "      <th>google_score</th>\n",
       "      <th>textblob_score</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.2500</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nice Designs</td>\n",
       "      <td>Praise on product</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>nice design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Looks good! Price?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>good ! price ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>fantastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lovely shoes. It looks like a hole in the welt...</td>\n",
       "      <td>Praise on product</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>lovely shoe . like hole welt stitch skip towar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Beautiful</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>beautiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How much for this piece?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>piece ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kindly DM what size available</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>kindly dm size available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beautiful colour</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>beautiful colour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  \\\n",
       "0                                             thanks   \n",
       "1                                       Nice Designs   \n",
       "2                                          How much?   \n",
       "3                                 Looks good! Price?   \n",
       "4                                          Fantastic   \n",
       "5  Lovely shoes. It looks like a hole in the welt...   \n",
       "6                                          Beautiful   \n",
       "7                           How much for this piece?   \n",
       "8                      kindly DM what size available   \n",
       "9                                   Beautiful colour   \n",
       "\n",
       "                             Categories  google_score  textblob_score  \\\n",
       "0                          Nice content           0.3           0.200   \n",
       "1                     Praise on product           0.8           0.600   \n",
       "2  Query about price or oder or product           0.0           0.200   \n",
       "3  Query about price or oder or product           0.3           0.875   \n",
       "4                          Nice content           0.9           0.400   \n",
       "5                     Praise on product           0.1           0.250   \n",
       "6                          Nice content           0.8           0.850   \n",
       "7  Query about price or oder or product          -0.1           0.200   \n",
       "8  Query about price or oder or product           0.0           0.500   \n",
       "9                          Nice content           0.8           0.850   \n",
       "\n",
       "   avg_score                                         clean_text  \n",
       "0     0.2500                                                     \n",
       "1     0.7000                                        nice design  \n",
       "2     0.1000                                                  ?  \n",
       "3     0.5875                                     good ! price ?  \n",
       "4     0.6500                                          fantastic  \n",
       "5     0.1750  lovely shoe . like hole welt stitch skip towar...  \n",
       "6     0.8250                                          beautiful  \n",
       "7     0.0500                                            piece ?  \n",
       "8     0.2500                           kindly dm size available  \n",
       "9     0.8250                                   beautiful colour  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the wordcloud library\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# # useless_list = ['hi', 'seamlessbespoke', 'll', 'congratulation', 'congrats']\n",
    "\n",
    "# # Join the different processed titles together.\n",
    "# long_string = ' '.join(list(df['clean_text'].values))\n",
    "\n",
    "# # long_string_list = long_string.split()\n",
    "# # long_string_list = [w for w in long_string_list if w not in useless_list]\n",
    "# # long_string = ' '.join(long_string_list)\n",
    "\n",
    "# # Create a WordCloud object\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# # Generate a word cloud\n",
    "# wordcloud.generate(long_string)\n",
    "\n",
    "# # Visualize the word cloud\n",
    "# wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the library with the CountVectorizer method\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_style('whitegrid')\n",
    "# %matplotlib inline \n",
    "\n",
    "# # Helper function\n",
    "# def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     words = count_vectorizer.get_feature_names()\n",
    "#     total_counts = np.zeros(len(words))\n",
    "#     for t in count_data:\n",
    "#         total_counts+=t.toarray()[0]\n",
    "    \n",
    "#     count_dict = (zip(words, total_counts))\n",
    "#     count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:50]\n",
    "#     words = [w[0] for w in count_dict]\n",
    "#     counts = [w[1] for w in count_dict]\n",
    "#     x_pos = np.arange(len(words)) \n",
    "    \n",
    "#     plt.figure(2, figsize=(15, 15/1.6180))\n",
    "#     plt.subplot(title='50 most common words')\n",
    "#     sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "#     sns.barplot(x_pos, counts, palette='husl')\n",
    "#     plt.xticks(x_pos, words, rotation=90) \n",
    "#     plt.xlabel('words')\n",
    "#     plt.ylabel('counts')\n",
    "#     plt.show()\n",
    "\n",
    "# # Initialise the count vectorizer with the English stop words    \n",
    "# count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# # Fit and transform the processed titles\n",
    "# count_data = count_vectorizer.fit_transform(df['clean_text'])\n",
    "# # Visualise the 10 most common words\n",
    "# plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# # Load the LDA model from sk-learn\n",
    "# from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# # Helper function\n",
    "# def print_topics(model, count_vectorizer, n_top_words):\n",
    "#     words = count_vectorizer.get_feature_names()\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(\"\\nTopic #%d:\" % (topic_idx+1))\n",
    "#         print(\" \".join([words[i]\n",
    "#                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# # Tweak the two parameters below\n",
    "# number_topics = 3\n",
    "# number_words = 10\n",
    "\n",
    "# # Create and fit the LDA model\n",
    "# lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "# lda.fit(count_data)\n",
    "\n",
    "# # Print the topics found by the LDA model\n",
    "# print(\"Topics found via LDA:\")\n",
    "# print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from pyLDAvis import sklearn as sklearn_lda\n",
    "# import pickle \n",
    "# import pyLDAvis\n",
    "\n",
    "# LDAvis_data_filepath = ''.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # # this is a bit time consuming - make the if statement True\n",
    "# # # if you want to execute visualization prep yourself\n",
    "\n",
    "# if 1 == 1:\n",
    "#     LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(LDAvis_data_filepath, 'wb') as f:\n",
    "#     pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# # load the pre-prepared pyLDAvis data from disk\n",
    "# with open(LDAvis_data_filepath, 'rb') as f:\n",
    "#     LDAvis_prepared = pickle.load(f)\n",
    "# pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for post in df['clean_text']:\n",
    "    x = word_tokenize(post)\n",
    "    x = [w for w in x if w.isalpha()]\n",
    "    tokens.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>Categories</th>\n",
       "      <th>google_score</th>\n",
       "      <th>textblob_score</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.2500</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nice Designs</td>\n",
       "      <td>Praise on product</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>nice design</td>\n",
       "      <td>[nice, design]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Looks good! Price?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>good ! price ?</td>\n",
       "      <td>[good, price]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>[fantastic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             comments                            Categories  google_score  \\\n",
       "0              thanks                          Nice content           0.3   \n",
       "1        Nice Designs                     Praise on product           0.8   \n",
       "2           How much?  Query about price or oder or product           0.0   \n",
       "3  Looks good! Price?  Query about price or oder or product           0.3   \n",
       "4           Fantastic                          Nice content           0.9   \n",
       "\n",
       "   textblob_score  avg_score      clean_text          tokens  \n",
       "0           0.200     0.2500                              []  \n",
       "1           0.600     0.7000     nice design  [nice, design]  \n",
       "2           0.200     0.1000               ?              []  \n",
       "3           0.875     0.5875  good ! price ?   [good, price]  \n",
       "4           0.400     0.6500       fantastic     [fantastic]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = tokens\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare bi-grams and tri-grams\n",
    "tokens = df['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare objects for LDA gensim implementation\n",
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running LDA\n",
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 617 ms\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "num_topics = 2\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: \n",
      "0.084*\"great\" + 0.081*\"beautiful\" + 0.061*\"lovely\" + 0.056*\"wow\" + 0.045*\"perfect\" + 0.045*\"cool\" + 0.045*\"awesome\" + 0.040*\"order\" + 0.040*\"suit\" + 0.033*\"detail\" + 0.030*\"linen\" + 0.026*\"purchase\" + 0.026*\"stylish\" + 0.026*\"dm_price\" + 0.020*\"go\" + 0.020*\"still\" + 0.020*\"yanko\" + 0.020*\"nice_work\" + 0.020*\"classy\" + 0.020*\"pm_price\"\n",
      "\n",
      "Topic #1: \n",
      "0.176*\"nice\" + 0.092*\"love\" + 0.076*\"like\" + 0.060*\"price\" + 0.055*\"good\" + 0.050*\"amazing\" + 0.045*\"photo\" + 0.045*\"shoe\" + 0.030*\"fabric\" + 0.025*\"could\" + 0.024*\"beautiful\" + 0.022*\"great\" + 0.021*\"picture\" + 0.020*\"right\" + 0.020*\"best\" + 0.020*\"top\" + 0.020*\"amaze\" + 0.020*\"collar\" + 0.018*\"wow\" + 0.015*\"gorgeous\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#looking at topics\n",
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(\"Topic #\"+str(i)+\": \")\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyee\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el358019029722775286415325104\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el358019029722775286415325104_data = {\"mdsDat\": {\"x\": [0.23557543754577637, -0.23557543754577637], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [54.90615463256836, 45.09384536743164]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [33.0, 14.0, 10.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 7.0, 7.0, 8.0, 17.0, 5.0, 20.0, 17.0, 5.0, 4.0, 7.0, 4.0, 5.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 33.754310607910156, 14.471624374389648, 10.615094184875488, 9.650956153869629, 8.686812400817871, 5.794413089752197, 4.830288410186768, 3.866152763366699, 3.866152763366699, 3.8661534786224365, 2.90201735496521, 2.90201735496521, 2.902017593383789, 2.9020183086395264, 11.442726135253906, 8.533546447753906, 3.7535219192504883, 3.7535226345062256, 17.590272903442383, 2.90201735496521, 2.90201735496521, 1.937881350517273, 3.978782892227173, 1.9378821849822998, 1.9378814697265625, 1.086378574371338, 0.973738431930542, 0.9737381935119629, 3.445817708969116, 4.621832847595215, 4.204038619995117, 1.086378574371338, 6.250260829925537, 5.210285186767578, 4.170295238494873, 3.130321979522705, 3.1303205490112305, 3.130321979522705, 3.1303224563598633, 3.130321741104126, 3.130321979522705, 3.1303205490112305, 3.130321741104126, 7.105138778686523, 4.048813819885254, 7.047268867492676, 7.047267436981201, 3.0088303089141846, 3.0088303089141846, 3.008831024169922, 2.977452039718628, 9.585724830627441, 4.785183906555176, 6.248576641082764, 4.048813343048096, 13.165663719177246, 12.715002059936523, 8.783623695373535, 2.0903398990631104, 2.0903396606445312, 1.9688429832458496, 2.0903398990631104, 3.0088303089141846, 2.0903396606445312, 2.8862850666046143], \"Term\": [\"nice\", \"like\", \"good\", \"price\", \"lovely\", \"amazing\", \"photo\", \"perfect\", \"order\", \"cool\", \"awesome\", \"shoe\", \"great\", \"detail\", \"love\", \"beautiful\", \"fabric\", \"purchase\", \"suit\", \"stylish\", \"linen\", \"could\", \"go\", \"classy\", \"pm_price\", \"still\", \"nice_work\", \"yanko\", \"great_shot\", \"available\", \"nice\", \"like\", \"good\", \"amazing\", \"photo\", \"fabric\", \"could\", \"best\", \"top\", \"right\", \"u\", \"buy\", \"check\", \"gorgeous\", \"price\", \"shoe\", \"collar\", \"amaze\", \"love\", \"super\", \"colour\", \"ready_wear\", \"picture\", \"jacket\", \"get\", \"stuff\", \"online\", \"product\", \"wow\", \"beautiful\", \"great\", \"dm_price\", \"order\", \"detail\", \"purchase\", \"yanko\", \"great_shot\", \"still\", \"go\", \"classy\", \"nice_work\", \"available\", \"pm_price\", \"perfect\", \"stylish\", \"cool\", \"awesome\", \"ship\", \"available_online\", \"fantastic\", \"cost\", \"lovely\", \"linen\", \"suit\", \"dm_price\", \"great\", \"beautiful\", \"wow\", \"online\", \"product\", \"stuff\", \"get\", \"picture\", \"jacket\", \"love\"], \"Total\": [33.0, 14.0, 10.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 7.0, 7.0, 8.0, 17.0, 5.0, 20.0, 17.0, 5.0, 4.0, 7.0, 4.0, 5.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 33.76470947265625, 14.482024192810059, 10.625494003295898, 9.661355972290039, 8.697212219238281, 5.804812908172607, 4.840688228607178, 3.8765525817871094, 3.8765525817871094, 3.8765532970428467, 2.91241717338562, 2.91241717338562, 2.912417411804199, 2.9124181270599365, 11.600356101989746, 8.709272384643555, 3.885408401489258, 3.885409355163574, 20.476558685302734, 3.952369213104248, 3.952369451522827, 2.9882333278656006, 6.987613201141357, 4.02822208404541, 4.028221130371094, 3.0552215576171875, 3.0640783309936523, 3.064077854156494, 12.22944164276123, 17.336833953857422, 17.369701385498047, 5.135191917419434, 6.259902000427246, 5.219926357269287, 4.179936408996582, 3.139963388442993, 3.1399619579315186, 3.139963388442993, 3.1399638652801514, 3.139963150024414, 3.139963388442993, 3.1399619579315186, 3.139963150024414, 7.286375999450684, 4.171082019805908, 7.282163143157959, 7.282162189483643, 3.1310982704162598, 3.1310982704162598, 3.131098985671997, 3.128805160522461, 10.359679222106934, 5.188910007476807, 7.223901271820068, 5.135191917419434, 17.369701385498047, 17.336833953857422, 12.22944164276123, 3.0640783309936523, 3.064077854156494, 3.0552215576171875, 4.028221130371094, 6.987613201141357, 4.02822208404541, 20.476558685302734], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.5992000102996826, 0.598800003528595, 0.5985999703407288, 0.5985000133514404, 0.5982999801635742, 0.5978000164031982, 0.5974000096321106, 0.5968999862670898, 0.5968999862670898, 0.5968999862670898, 0.5960000157356262, 0.5960000157356262, 0.5960000157356262, 0.5960000157356262, 0.5859000086784363, 0.579200029373169, 0.5649999976158142, 0.5649999976158142, 0.44760000705718994, 0.2906000018119812, 0.2906000018119812, 0.1665000021457672, 0.036400001496076584, -0.13220000267028809, -0.13220000267028809, -0.4345000088214874, -0.5468000173568726, -0.5468000173568726, -0.6671000123023987, -0.7225000262260437, -0.819100022315979, -0.9537000060081482, 0.7949000000953674, 0.7946000099182129, 0.7940999865531921, 0.7932999730110168, 0.7932999730110168, 0.7932999730110168, 0.7932999730110168, 0.7932999730110168, 0.7932999730110168, 0.7932999730110168, 0.7932999730110168, 0.7712000012397766, 0.766700029373169, 0.7635999917984009, 0.7635999917984009, 0.756600022315979, 0.756600022315979, 0.756600022315979, 0.7468000054359436, 0.7188000082969666, 0.715399980545044, 0.6514000296592712, 0.5587000250816345, 0.5192999839782715, 0.4864000082015991, 0.46549999713897705, 0.414000004529953, 0.414000004529953, 0.3569999933242798, 0.1404000073671341, -0.04619999974966049, 0.1404000073671341, -1.1628999710083008], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.7364000082015991, -2.5833001136779785, -2.8933000564575195, -2.9885001182556152, -3.0936999320983887, -3.4986000061035156, -3.6805999279022217, -3.9033000469207764, -3.9033000469207764, -3.9033000469207764, -4.190100193023682, -4.190100193023682, -4.190100193023682, -4.190100193023682, -2.81820011138916, -3.1115000247955322, -3.932800054550171, -3.932800054550171, -2.388200044631958, -4.190100193023682, -4.190100193023682, -4.593900203704834, -3.8745999336242676, -4.593900203704834, -4.593900203704834, -5.172699928283691, -5.282100200653076, -5.282100200653076, -4.018400192260742, -3.7246999740600586, -3.819499969482422, -5.172699928283691, -3.2260000705718994, -3.4079999923706055, -3.63070011138916, -3.9175000190734863, -3.9175000190734863, -3.9175000190734863, -3.9175000190734863, -3.9175000190734863, -3.9175000190734863, -3.9175000190734863, -3.9175000190734863, -3.0978000164031982, -3.6602001190185547, -3.1059999465942383, -3.1059999465942383, -3.9570999145507812, -3.9570999145507812, -3.9570999145507812, -3.967600107192993, -2.7983999252319336, -3.4930999279022217, -3.226300001144409, -3.6602001190185547, -2.4809999465942383, -2.515899896621704, -2.8857998847961426, -4.321300029754639, -4.321300029754639, -4.381199836730957, -4.321300029754639, -3.9570999145507812, -4.321300029754639, -3.998699903488159]}, \"token.table\": {\"Topic\": [1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2], \"Freq\": [1.0294924974441528, 1.0350513458251953, 0.9554255604743958, 0.9581302404403687, 0.9612529873847961, 0.28840330243110657, 0.7498485445976257, 1.0318446159362793, 1.0300722122192383, 1.0300720930099487, 0.9554252028465271, 1.0294928550720215, 0.7590383291244507, 0.2530127763748169, 0.9612528681755066, 0.9588325023651123, 1.032910943031311, 0.9578679203987122, 0.19473469257354736, 0.7789387702941895, 1.0336250066757202, 0.9581300616264343, 0.4964970648288727, 0.4964970648288727, 0.9554250240325928, 1.0352460145950317, 1.0300718545913696, 0.23028606176376343, 0.7484296560287476, 0.9554255604743958, 0.49649694561958313, 0.49649694561958313, 0.9667156934738159, 0.9635934829711914, 0.8790539503097534, 0.14650899171829224, 0.0965280830860138, 0.9652808308601379, 1.0069684982299805, 0.9554251432418823, 0.32636240124702454, 0.6527248024940491, 0.9584814310073853, 0.9606970548629761, 1.0348143577575684, 0.5724415183067322, 0.4293311536312103, 0.9554252028465271, 0.9482467770576477, 0.3263624608516693, 0.6527249217033386, 0.9569523334503174, 0.6692917943000793, 0.3346458971500397, 1.0318444967269897, 0.9581302404403687, 1.0333813428878784, 0.9554251432418823, 0.3273085057735443, 0.6546170115470886, 0.9589837789535522, 0.13842935860157013, 0.8305761218070984, 0.7590383887290955, 0.2530128061771393, 1.0318446159362793, 1.0300722122192383, 0.24530965089797974, 0.7359289526939392, 0.9554251432418823], \"Term\": [\"amaze\", \"amazing\", \"available\", \"available_online\", \"awesome\", \"beautiful\", \"beautiful\", \"best\", \"buy\", \"check\", \"classy\", \"collar\", \"colour\", \"colour\", \"cool\", \"cost\", \"could\", \"detail\", \"dm_price\", \"dm_price\", \"fabric\", \"fantastic\", \"get\", \"get\", \"go\", \"good\", \"gorgeous\", \"great\", \"great\", \"great_shot\", \"jacket\", \"jacket\", \"like\", \"linen\", \"love\", \"love\", \"lovely\", \"lovely\", \"nice\", \"nice_work\", \"online\", \"online\", \"order\", \"perfect\", \"photo\", \"picture\", \"picture\", \"pm_price\", \"price\", \"product\", \"product\", \"purchase\", \"ready_wear\", \"ready_wear\", \"right\", \"ship\", \"shoe\", \"still\", \"stuff\", \"stuff\", \"stylish\", \"suit\", \"suit\", \"super\", \"super\", \"top\", \"u\", \"wow\", \"wow\", \"yanko\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el358019029722775286415325104\", ldavis_el358019029722775286415325104_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el358019029722775286415325104\", ldavis_el358019029722775286415325104_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el358019029722775286415325104\", ldavis_el358019029722775286415325104_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "LDAvis_data_filepath = ''.join('./ldavis_prepared_'+str(num_topics))\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    pickle.dump(vis, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model[corpus[0]]\n",
    "topics_list = [lda_model[corpus[i]] for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.3003802), (1, 0.6996198)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.9950494)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.72617346), (1, 0.27382657)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.9950494)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.9966887)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.990196)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9950494)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99668866)],\n",
       " [(1, 0.9950494)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.990196)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019593)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.9966887)],\n",
       " [(0, 0.661462), (1, 0.33853793)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9966887)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9966887)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99504936)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.42931518), (1, 0.57068473)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9966887)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.50015056), (1, 0.49984938)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.9901961)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.334049), (1, 0.6659511)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.35493392), (1, 0.6450661)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.990196)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.9966887)],\n",
       " [(1, 0.99751234)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.9950495)],\n",
       " [(0, 0.990196)],\n",
       " [(0, 0.9950494)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.9966887)],\n",
       " [(1, 0.990196)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.99019593)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.99019605)],\n",
       " [(1, 0.9950495)],\n",
       " [(1, 0.99019605)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.99751234)],\n",
       " [(1, 0.9950495)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.99019605)],\n",
       " [(0, 0.50005287), (1, 0.4999472)],\n",
       " [(1, 0.99019605)],\n",
       " [(1, 0.9901961)],\n",
       " [(0, 0.5), (1, 0.5)],\n",
       " [(0, 0.9950495)],\n",
       " [(1, 0.9901961)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'] = topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>Categories</th>\n",
       "      <th>google_score</th>\n",
       "      <th>textblob_score</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.2500</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, 0.5), (1, 0.5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nice Designs</td>\n",
       "      <td>Praise on product</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>nice design</td>\n",
       "      <td>[nice, design]</td>\n",
       "      <td>[(1, 0.9901961)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(0, 0.5), (1, 0.5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Looks good! Price?</td>\n",
       "      <td>Query about price or oder or product</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>good ! price ?</td>\n",
       "      <td>[good, price]</td>\n",
       "      <td>[(1, 0.9950495)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>Nice content</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>[fantastic]</td>\n",
       "      <td>[(0, 0.990196)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             comments                            Categories  google_score  \\\n",
       "0              thanks                          Nice content           0.3   \n",
       "1        Nice Designs                     Praise on product           0.8   \n",
       "2           How much?  Query about price or oder or product           0.0   \n",
       "3  Looks good! Price?  Query about price or oder or product           0.3   \n",
       "4           Fantastic                          Nice content           0.9   \n",
       "\n",
       "   textblob_score  avg_score      clean_text          tokens  \\\n",
       "0           0.200     0.2500                              []   \n",
       "1           0.600     0.7000     nice design  [nice, design]   \n",
       "2           0.200     0.1000               ?              []   \n",
       "3           0.875     0.5875  good ! price ?   [good, price]   \n",
       "4           0.400     0.6500       fantastic     [fantastic]   \n",
       "\n",
       "                  topic  \n",
       "0  [(0, 0.5), (1, 0.5)]  \n",
       "1      [(1, 0.9901961)]  \n",
       "2  [(0, 0.5), (1, 0.5)]  \n",
       "3      [(1, 0.9950495)]  \n",
       "4       [(0, 0.990196)]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('insta_comment_noemo_topic.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
